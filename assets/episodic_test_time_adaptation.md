# Episodic Test-Time Adaptation

* `NeurIPS-2022` **[Test-Time Prompt Tuning for Zero-Shot Generalization in Vision-Language Models](https://arxiv.org/pdf/2209.07511)** [![Star](https://img.shields.io/github/stars/azshue/TPT.svg?style=social&label=Star)](https://github.com/azshue/TPT)
    * Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anandkumar, Chaowei Xiao
* `NeurIPS-2022` **[ReCo: Retrieve and Co-segment for Zero-shot Transfer](https://arxiv.org/pdf/2206.07045)** [![Star](https://img.shields.io/github/stars/NoelShin/reco.svg?style=social&label=Star)](https://github.com/NoelShin/reco)
    * Gyungin Shin, Weidi Xie, Samuel Albanie
* `CVPR-2023` **[Improving Zero-shot Generalization and Robustness of Multi-modal Models](https://arxiv.org/pdf/2212.01758)** [![Star](https://img.shields.io/github/stars/gyhandy/Hierarchy-CLIP.svg?style=social&label=Star)](https://github.com/gyhandy/Hierarchy-CLIP)
    * Yunhao Ge, Jie Ren, Andrew Gallagher, Yuxiao Wang, Ming-Hsuan Yang, Hartwig Adam, Laurent Itti, Balaji Lakshminarayanan, Jiaping Zhao
* `CVPR-2023` **[Texts as Images in Prompt Tuning for Multi-Label Image Recognition](https://openaccess.thecvf.com/content/CVPR2023/papers/Guo_Texts_as_Images_in_Prompt_Tuning_for_Multi-Label_Image_Recognition_CVPR_2023_paper.pdf)** [![Star](https://img.shields.io/github/stars/guozix/TaI-DPT.svg?style=social&label=Star)](https://github.com/guozix/TaI-DPT)
    * Zixian Guo, Bowen Dong, Zhilong Ji, Jinfeng Bai, Yiwen Guo, Wangmeng Zuo
* `ICCV-2023` **[Diverse Data Augmentation with Diffusions for Effective Test-time Prompt Tuning](https://arxiv.org/pdf/2308.06038)** [![Star](https://img.shields.io/github/stars/chunmeifeng/DiffTPT.svg?style=social&label=Star)](https://github.com/chunmeifeng/DiffTPT)
    * Chun-Mei Feng, Kai Yu, Yong Liu, Salman Khan, Wangmeng Zuo
* `ACMMM-2023` **[VPA: Fully Test-Time Visual Prompt Adaptation](https://arxiv.org/pdf/2309.15251)**
    * Jiachen Sun, Mark Ibrahim, Melissa Hall, Ivan Evtimov, Z. Morley Mao, Cristian Canton Ferrer, Caner Hazirbas
* `NeurIPS-2023` **[Diffusion-TTA: Test-time Adaptation of Discriminative Models via Generative Feedback](https://arxiv.org/pdf/2311.16102)** [![Star](https://img.shields.io/github/stars/mihirp1998/Diffusion-TTA.svg?style=social&label=Star)](https://github.com/mihirp1998/Diffusion-TTA)
    * Mihir Prabhudesai, Tsung-Wei Ke, Alexander C. Li, Deepak Pathak, Katerina Fragkiadaki
* `NeurIPS-2023` **[Align Your Prompts: Test-Time Prompting with Distribution Alignment for Zero-Shot Generalization](https://arxiv.org/pdf/2311.01459)** [![Star](https://img.shields.io/github/stars/jameelhassan/PromptAlign.svg?style=social&label=Star)](https://github.com/jameelhassan/PromptAlign)
    * Jameel Hassan, Hanan Gani, Noor Hussein, Muhammad Uzair Khattak, Muzammal Naseer, Fahad Shahbaz Khan, Salman Khan
* `arXiv 2023` **[Cross-Modal Retrieval Meets Inference: Improving Zero-Shot Classification with Cross-Modal Retrieval](https://arxiv.org/pdf/2308.15273)**
    * Seongha Eom, Namgyu Ho, Jaehoon Oh, Se-Young Yun
* `arXiv 2023` **[Bootstrap Fine-Grained Vision-Language Alignment for Unified Zero-Shot Anomaly Localization](https://arxiv.org/pdf/2308.15939)**
    * Hanqiu Deng, Zhaoxiang Zhang, Jinan Bao, Xingyu Li
* `ICLR-2024` **[Test-Time Adaptation with CLIP Reward for Zero-Shot Generalization in Vision-Language Models](https://arxiv.org/pdf/2305.18010)** [![Star](https://img.shields.io/github/stars/mzhaoshuai/RLCF.svg?style=social&label=Star)](https://github.com/mzhaoshuai/RLCF)
    * Shuai Zhao, Xiaohan Wang, Linchao Zhu, Yi Yang
* `ICLR-2024` **[C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via Text Feature Dispersion](https://arxiv.org/pdf/2403.14119)** [![Star](https://img.shields.io/github/stars/hee-suk-yoon/C-TPT.svg?style=social&label=Star)](https://github.com/hee-suk-yoon/C-TPT)
    * Hee Suk Yoon, Eunseop Yoon, Joshua Tian Jin Tee, Mark Hasegawa-Johnson, Yingzhen Li, Chang D. Yoo
* `ICLR-2024` **[PerceptionCLIP: Visual Classification by Inferring and Conditioning on Contexts](https://openreview.net/pdf?id=2Oiee202rd)** [![Star](https://img.shields.io/github/stars/umd-huang-lab/perceptionCLIP.svg?style=social&label=Star)](https://github.com/umd-huang-lab/perceptionCLIP)
    * Bang An, Sicheng Zhu, Michael-Andrei Panaitescu-Liess, Chaithanya Kumar Mummadi, Furong Huang
* `ICLR-2024` **[Follow-Up Differential Descriptions: Language Models Resolve Ambiguities for Image Classification](https://arxiv.org/pdf/2311.07593)** [![Star](https://img.shields.io/github/stars/BatsResearch/fudd.svg?style=social&label=Star)](https://github.com/BatsResearch/fudd)
    * Reza Esfandiarpoor, Stephen H. Bach
* `CVPR-2024` **[On the Test-Time Zero-Shot Generalization of Vision-Language Models: Do We Really Need Prompt Learning?](https://arxiv.org/pdf/2405.02266)** [![Star](https://img.shields.io/github/stars/MaxZanella/MTA.svg?style=social&label=Star)](https://github.com/MaxZanella/MTA)
    * Maxime Zanella, Ismail Ben Ayed
* `CVPR-2024` **[Test-Time Zero-Shot Temporal Action Localization](https://arxiv.org/pdf/2404.05426)** [![Star](https://img.shields.io/github/stars/benedettaliberatori/T3AL.svg?style=social&label=Star)](https://github.com/benedettaliberatori/T3AL)
    * Benedetta Liberatori, Alessandro Conti, Paolo Rota, Yiming Wang, Elisa Ricci
* `CVPR-2024` **[Leveraging Cross-Modal Neighbor Representation for Improved CLIP Classification](https://arxiv.org/pdf/2404.17753)** [![Star](https://img.shields.io/github/stars/YCaigogogo/CODER.svg?style=social&label=Star)](https://github.com/YCaigogogo/CODER)
    * Chao Yi, Lu Ren, De-Chuan Zhan, Han-Jia Ye
* `CVPR-2024` **[Grounding Everything: Emerging Localization Properties in Vision-Language Transformers](https://openaccess.thecvf.com/content/CVPR2024/papers/Bousselham_Grounding_Everything_Emerging_Localization_Properties_in_Vision-Language_Transformers_CVPR_2024_paper.pdf)** [![Star](https://img.shields.io/github/stars/WalBouss/GEM.svg?style=social&label=Star)](https://github.com/WalBouss/GEM)
    * Walid Bousselham, Felix Petersen, Vittorio Ferrari, Hilde Kuehne
* `CVPRW-2024` **[PromptSync: Bridging Domain Gaps in Vision-Language Models through Class-Aware Prototype Alignment and Discrimination](https://arxiv.org/pdf/2404.07520)**
    * Anant Khandelwal
* `WACV-2024` **[CLIP-DIY: CLIP Dense Inference Yields Open-Vocabulary Semantic Segmentation For-Free](https://arxiv.org/pdf/2309.14289)** [![Star](https://img.shields.io/github/stars/wysoczanska/clip-diy.svg?style=social&label=Star)](https://github.com/wysoczanska/clip-diy)
    * Monika Wysoczańska, Michaël Ramamonjisoa, Tomasz Trzciński, Oriane Siméoni
* `WACV-2024` **[DiffCLIP: Leveraging Stable Diffusion for Language Grounded 3D Classification](https://arxiv.org/pdf/2305.15957)** [![Star](https://img.shields.io/github/stars/SitianShen/DiffCLIP.svg?style=social&label=Star)](https://github.com/SitianShen/DiffCLIP)
    * Sitian Shen, Zilin Zhu, Linqian Fan, Harry Zhang, Xinxiao Wu
* `ICML-2024` **[Visual-Text Cross Alignment: Refining the Similarity Score in Vision-Language Models](https://arxiv.org/pdf/2406.02915)** [![Star](https://img.shields.io/github/stars/tmlr-group/WCA.svg?style=social&label=Star)](https://github.com/tmlr-group/WCA)
    * Jinhao Li, Haopeng Li, Sarah Erfani, Lei Feng, James Bailey, Feng Liu
* `IJCAI-2024` **[DTS-TPT: Dual Temporal-Sync Test-time Prompt Tuning for Zero-shot Activity Recognition](https://www.ijcai.org/proceedings/2024/0170.pdf)** [![Star](https://img.shields.io/github/stars/quhongyu/DTS-TPT.svg?style=social&label=Star)](https://github.com/quhongyu/DTS-TPT)
    * Rui Yan, Hongyu Qu, Xiangbo Shu, Wenbin Li, Jinhui Tang, Tieniu Tan
* `ECCV-2024` **[Robust Calibration of Large Vision-Language Adapters](https://arxiv.org/pdf/2407.13588)** [![Star](https://img.shields.io/github/stars/Bala93/CLIPCalib.svg?style=social&label=Star)](https://github.com/Bala93/CLIPCalib)
    * Balamurali Murugesan, Julio Silva-Rodriguez, Ismail Ben Ayed, Jose Dolz
* `ECCV-2024` **[Robust Calibration of Large Vision-Language Adapters](https://arxiv.org/pdf/2407.13588)** [![Star](https://img.shields.io/github/stars/Bala93/CLIPCalib.svg?style=social&label=Star)](https://github.com/Bala93/CLIPCalib)
    * Balamurali Murugesan, Julio Silva-Rodriguez, Ismail Ben Ayed, Jose Dolz
* `ECCV-2024` **[Explore the Potential of CLIP for Training-Free Open Vocabulary Semantic Segmentation](https://arxiv.org/pdf/2407.08268)** [![Star](https://img.shields.io/github/stars/leaves162/CLIPtrase.svg?style=social&label=Star)](https://github.com/leaves162/CLIPtrase)
    * Tong Shao, Zhuotao Tian, Hang Zhao, Jingyong Su
* `ECCV-2024` **[In Defense of Lazy Visual Grounding for Open-Vocabulary Semantic Segmentation](https://arxiv.org/pdf/2408.04961)** [![Star](https://img.shields.io/github/stars/dahyun-kang/lavg.svg?style=social&label=Star)](https://github.com/dahyun-kang/lavg)
    * Dahyun Kang, Minsu Cho
* `ACMMM-2024` **[WaveDN: A Wavelet-based Training-free Zero-shot Enhancement for Vision-Language Models](https://openreview.net/pdf?id=di99IjsY2T)**
    * Jiulin Li, Mengyu Yang, Ye Tian, Lanshan Zhang, Yongchun Lu, Jice Liu, Wendong Wang
* `NeurIPS-2024` **[Frustratingly Easy Test-Time Adaptation of Vision-Language Models](https://arxiv.org/pdf/2405.18330)** [![Star](https://img.shields.io/github/stars/FarinaMatteo/zero.svg?style=social&label=Star)](https://github.com/FarinaMatteo/zero)
    * Matteo Farina, Gianni Franchi, Giovanni Iacca, Massimiliano Mancini, Elisa Ricci
* `NeurIPS-2024` **[AWT: Transferring Vision-Language Models via Augmentation, Weighting, and Transportation](https://arxiv.org/pdf/2407.04603)** [![Star](https://img.shields.io/github/stars/MCG-NJU/AWT.svg?style=social&label=Star)](https://github.com/MCG-NJU/AWT)
    * Yuhan Zhu, Yuyang Ji, Zhiyu Zhao, Gangshan Wu, Limin Wang
* `ICAMechS-2024` **[IAD-CLIP: Vision-Language Models for Zero-Shot Industrial Anomaly Detection](https://ieeexplore.ieee.org/document/10818831)**
    * Zhuo Li, Yifei Ge, Qi Li, Lin Meng
* `EMNLP-2024` **[Enhancing Fine-Grained Image Classifications via Cascaded Vision Language Models](https://arxiv.org/pdf/2405.11301)**
    * Canshi Wei
* `arXiv 2024` **[Efficient Test-Time Prompt Tuning for Vision-Language Models](https://arxiv.org/pdf/2408.05775)**
    * Yuhan Zhu, Guozhen Zhang, Chen Xu, Haocheng Shen, Xiaoxin Chen, Gangshan Wu, Limin Wang
* `arXiv 2024` **[What Do You See? Enhancing Zero-Shot Image Classification with Multimodal Large Language Models](https://arxiv.org/pdf/2405.15668)**
    * Abdelrahman Abdelhamed, Mahmoud Afifi, Alec Go
* `arXiv 2024` **[S3: Synonymous Semantic Space for Improving Zero-Shot Generalization of Vision-Language Models](https://arxiv.org/pdf/2412.04925)**
    * Xiaojie Yin, Qilong Wang, Bing Cao, Qinghua Hu
* `arXiv 2024` **[Vocabulary-free Image Classification and Semantic Segmentation](https://arxiv.org/pdf/2404.10864)** [![Star](https://img.shields.io/github/stars/altndrr/vicss.svg?style=social&label=Star)](https://github.com/altndrr/vicss)
    * Alessandro Conti, Enrico Fini, Massimiliano Mancini, Paolo Rota, Yiming Wang, Elisa Ricci
* `arXiv 2024` **[AutoCLIP: Auto-tuning Zero-Shot Classifiers for VisionLanguage Models](https://arxiv.org/pdf/2309.16414)**
    * Jan Hendrik Metzen, Piyapat Saranrittichai, Chaithanya Kumar Mummadi
* `arXiv 2024` **[TAG: Guidance-free Open-Vocabulary Semantic Segmentation](https://arxiv.org/pdf/2403.11197)** [![Star](https://img.shields.io/github/stars/Valkyrja3607/TAG.svg?style=social&label=Star)](https://github.com/Valkyrja3607/TAG)
    * Yasufumi Kawano, Yoshimitsu Aoki
* `arXiv 2024` **[Training-Free Semantic Segmentation via LLM-Supervision](https://arxiv.org/pdf/2404.00701)**
    * Wenfang Sun, Yingjun Du, Gaowen Liu, Ramana Kompella, Cees GM Snoek
* `Openreview 2024` **[Don't Paint Everyone with the Same Brush: Adaptive Prompt Prototype Learning for Vision-Language Models](https://openreview.net/pdf?id=YG01CZDpCq)**
    * Zhi Chen
* `AAAI-2025` **[Spurious Feature Eraser: Stabilizing Test-Time Adaptation for Vision-Language Foundation Model](https://arxiv.org/pdf/2403.00376)** [![Star](https://img.shields.io/github/stars/MaHuanAAA/SEraser.svg?style=social&label=Star)](https://github.com/MaHuanAAA/SEraser)
    * Huan Ma, Yan Zhu, Changqing Zhang, Peilin Zhao, Baoyuan Wu, Long-Kai Huang, Qinghua Hu, Bingzhe Wu
* `WACV-2025` **[Test-Time Low Rank Adaptation via Confidence Maximization for Zero-Shot Generalization of Vision-Language Models](https://arxiv.org/pdf/2407.15913)** [![Star](https://img.shields.io/github/stars/Razaimam45/TTL-Test-Time-Low-Rank-Adaptation.svg?style=social&label=Star)](https://github.com/Razaimam45/TTL-Test-Time-Low-Rank-Adaptation)
    * Raza Imam, Hanan Gani, Muhammad Huzaifa, Karthik Nandakumar
* `WACV-2025` **[Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization with Vision-Language Models](https://arxiv.org/pdf/2403.12952)** [![Star](https://img.shields.io/github/stars/elaine-sui/TPS.svg?style=social&label=Star)](https://github.com/elaine-sui/TPS)
    * Elaine Sui, Xiaohan Wang, Serena Yeung-Levy
* `WACV-2025` **[Pay Attention to Your Neighbours: Training-Free Open-Vocabulary Semantic Segmentation](https://arxiv.org/pdf/2404.08181)** [![Star](https://img.shields.io/github/stars/sinahmr/NACLIP.svg?style=social&label=Star)](https://github.com/sinahmr/NACLIP)
    * Sina Hajimiri, Ismail Ben Ayed, Jose Dolz
* `ICLR-2025` **[RA-TTA: Retrieval-Augmented Test-Time Adaptation for Vision-Language Models](https://openreview.net/pdf?id=V3zobHnS61)** [![Star](https://img.shields.io/github/stars/kaist-dmlab/RA-TTA.svg?style=social&label=Star)](https://github.com/kaist-dmlab/RA-TTA)
    * Youngjun Lee, Doyoung Kim, Junhyeok Kang, Jihwan Bang, Hwanjun Song, Jae-Gil Lee
* `CVPR-2025` **[CLIP is Strong Enough to Fight Back: Test-time Counterattacks towards Zero-shot Adversarial Robustness of CLIP](https://arxiv.org/pdf/2503.03613)** [![Star](https://img.shields.io/github/stars/Sxing2/CLIP-Test-time-Counterattacks.svg?style=social&label=Star)](https://github.com/Sxing2/CLIP-Test-time-Counterattacks)
    * Songlong Xing, Zhengyu Zhao, Nicu Sebe
* `CVPR-2025` **[TAPT: Test-Time Adversarial Prompt Tuning for Robust Inference in Vision-Language Models](https://arxiv.org/pdf/2411.13136)** [![Star](https://img.shields.io/github/stars/xinwong/TAPT.svg?style=social&label=Star)](https://github.com/xinwong/TAPT)
    * Xin Wang, Kai Chen, Jiaming Zhang, Jingjing Chen, Xingjun Ma
* `CVPR-2025` **[SCAP: Transductive Test-Time Adaptation via Supportive Clique-based Attribute Prompting](https://arxiv.org/pdf/2503.12866)** [![Star](https://img.shields.io/github/stars/zhoujiahuan1991/CVPR2025-SCAP.svg?style=social&label=Star)](https://github.com/zhoujiahuan1991/CVPR2025-SCAP)
    * Chenyu Zhang, Kunlun Xu, Zichen Liu, Yuxin Peng, Jiahuan Zhou
* `CVPR-2025` **[O-TPT: Orthogonality Constraints for Calibrating Test-time Prompt Tuning in Vision-Language Models](https://arxiv.org/pdf/2503.12096)** [![Star](https://img.shields.io/github/stars/ashshaksharifdeen/O-TPT.svg?style=social&label=Star)](https://github.com/ashshaksharifdeen/O-TPT)
    * Ashshak Sharifdeen, Muhammad Akhtar Munir, Sanoojan Baliah, Salman Khan, Muhammad Haris Khan
* `CVPR-2025` **[R-TPT: Improving Adversarial Robustness of Vision-Language Models through Test-Time Prompt Tuning](https://arxiv.org/pdf/2504.11195)** [![Star](https://img.shields.io/github/stars/TomSheng21/R-TPT.svg?style=social&label=Star)](https://github.com/TomSheng21/R-TPT)
    * Lijun Sheng, Jian Liang, Zilei Wang, Ran He
* `CVPR-2025` **[Realistic Test-Time Adaptation of Vision-Language Models](https://arxiv.org/pdf/2501.03729)** [![Star](https://img.shields.io/github/stars/MaxZanella/StatA.svg?style=social&label=Star)](https://github.com/MaxZanella/StatA)
    * Maxime Zanella, Clément Fuchs, Christophe De Vleeschouwer, Ismail Ben Ayed
* `CVPR-2025` **[SPARC: Score Prompting and Adaptive Fusion for Zero-Shot Multi-Label Recognition in Vision-Language Models](https://arxiv.org/pdf/2502.16911)** [![Star](https://img.shields.io/github/stars/kjmillerCURIS/SPARC.svg?style=social&label=Star)](https://github.com/kjmillerCURIS/SPARC)
    * Kevin Miller, Samarth Mishra, Aditya Gangrade, Kate Saenko, Venkatesh Saligrama
* `CVPRW-2025` **[TLAC: Two-stage LMM Augmented CLIP for Zero-Shot Classification](https://arxiv.org/pdf/2503.12206)** [![Star](https://img.shields.io/github/stars/ans92/TLAC.svg?style=social&label=Star)](https://github.com/ans92/TLAC)
    * Ans Munir, Faisal Z Qureshi, Muhammad Haris Khan, Mohsen Ali
* `ICML-2025` **[From Local Details to Global Context: Advancing Vision-Language Models with Attention-based Selection](https://arxiv.org/pdf/2505.13233)** [![Star](https://img.shields.io/github/stars/BIT-DA/ABS.svg?style=social&label=Star)](https://github.com/BIT-DA/ABS)
    * Lincan Cai, Jingxuan Kang, Shuang Li, Wenxuan Ma, Binhui Xie, Zhida Qin, Jian Liang
* `ICML-2025` **[GS-Bias: Global-Spatial Bias Learner for Single-Image Test-Time Adaptation of Vision-Language Models](https://arxiv.org/pdf/2507.11969)** [![Star](https://img.shields.io/github/stars/hzhxmu/GS-Bias.svg?style=social&label=Star)](https://github.com/hzhxmu/GS-Bias)
    * Zhaohong Huang, Yuxin Zhang, Jingjing Xie, Fei Chao, Rongrong Ji
* `ICMLW-2025` **[LoRA-TTT: Low-Rank Test-Time Training for Vision-Language Models](https://arxiv.org/pdf/2502.02069)**
    * Yuto Kojima, Jiarui Xu, Xueyan Zou, Xiaolong Wang
* `ICIP-2025` **[Test-time Vocabulary Adaptation for Language-driven Object Detection](https://arxiv.org/pdf/2506.00333)**
    * Mingxuan Liu, Tyler L. Hayes, Massimiliano Mancini, Elisa Ricci, Riccardo Volpi, Gabriela Csurka
* `EAAI-2025` **[Enhancing Crop Disease Recognition via Prompt Learning-based Progressive Mixup and Contrastive Language-Image Pre-training Dynamic Calibration](https://dl.acm.org/doi/10.1016/j.engappai.2025.110805)**
    * Hao Chen, Haidong Li, Jinling Zhao, Chao Ruan, Linsheng Huang
* `IJCV-2025` **[Diffusion-Enhanced Test-time Adaptation with Text and Image Augmentation](https://arxiv.org/pdf/2412.09706)** [![Star](https://img.shields.io/github/stars/chunmeifeng/DiffTPT.svg?style=social&label=Star)](https://github.com/chunmeifeng/DiffTPT)
    * Chun-Mei Feng, Yuanyang He, Jian Zou, Salman Khan, Huan Xiong, Zhen Li, Wangmeng Zuo, Rick Siow Mong Goh, Yong Liu
* `arXiv 2025` **[Leveraging Vision-Language Embeddings for Zero-Shot Learning in Histopathology Images](https://arxiv.org/pdf/2503.10731)**
    * Md Mamunur Rahaman, Ewan K. A. Millar, Erik Meijering
* `arXiv 2025` **[Noise is an Efficient Learner for Zero-Shot Vision-Language Models](https://arxiv.org/pdf/2502.06019)**
    * Raza Imam, Asif Hanif, Jian Zhang, Khaled Waleed Dawoud, Yova Kementchedjhieva, Mohammad Yaqub
* `arXiv 2025` **[Bidirectional Prototype-Reward co-Evolution for Test-Time Adaptation of Vision-Language Models](https://arxiv.org/pdf/2503.09394)**
    * Xiaozhen Qiao, Peng Huang, Jiakang Yuan, Xianda Guo, Bowen Ye, Zhe Sun, Xuelong Li
* `arXiv 2025` **[Test-time Loss Landscape Adaptation for Zero-Shot Generalization in Vision-Language Models](https://arxiv.org/pdf/2501.18864)**
    * Aodi Li, Liansheng Zhuang, Xiao Long, Minghong Yao, Shafei Wang
